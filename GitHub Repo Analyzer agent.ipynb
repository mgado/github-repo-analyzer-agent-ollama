{"cells":[{"cell_type":"markdown","metadata":{"id":"Cgmii7eCDQtq"},"source":["# Install dependencies"]},{"cell_type":"markdown","metadata":{"id":"bKqfXvnIE0AN"},"source":["Download and Set Up Ollama (downloads the Ollama installation script, executes it and starts the Ollama server as a background process.)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46967,"status":"ok","timestamp":1755957058110,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"_2hJJ9e0FPGX","outputId":"6af8a1f7-21e0-4f6a-d582-8e9177e14d40"},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully ran: curl -fsSL https://ollama.com/install.sh | sh\n","‚úÖ Ollama server started in the background.\n"]}],"source":["import os\n","import asyncio\n","\n","async def run_command_async(cmd):\n","  \"\"\"Runs a shell command asynchronously.\"\"\"\n","  process = await asyncio.create_subprocess_shell(\n","      cmd,\n","      stdout=asyncio.subprocess.PIPE,\n","      stderr=asyncio.subprocess.PIPE\n","  )\n","  stdout, stderr = await process.communicate()\n","  if process.returncode != 0:\n","    print(f\"Error running command: {cmd}\")\n","    print(f\"Stderr: {stderr.decode()}\")\n","  else:\n","    print(f\"Successfully ran: {cmd}\")\n","\n","async def setup_ollama():\n","  \"\"\"Downloads, installs, and starts the Ollama server.\"\"\"\n","  await run_command_async(\"curl -fsSL https://ollama.com/install.sh | sh\")\n","  # Start Ollama in the background using nohup to keep it running\n","  os.system('nohup ollama serve \u003e ollama.log 2\u003e\u00261 \u0026')\n","  # Give the server a moment to initialize\n","  await asyncio.sleep(5)\n","  print(\"‚úÖ Ollama server started in the background.\")\n","\n","# Run the asynchronous setup\n","await setup_ollama()"]},{"cell_type":"markdown","metadata":{"id":"ZE5euNM2E-h9"},"source":["Install all the necessary Python libraries for the project."]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":12105,"status":"ok","timestamp":1755956289795,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"YlttBlJ9EypA"},"outputs":[],"source":["!pip install gradio requests ollama -q"]},{"cell_type":"markdown","metadata":{"id":"R77114I1DOfR"},"source":["# Create Project files"]},{"cell_type":"markdown","metadata":{"id":"ruiW7FoVGVbm"},"source":["\n","\n","* **config.py**: A list of models (to be used for the dropdown)\n","* **ollama_utils.py**: This module handles all direct interactions with the Ollama service.\n","* **github_utils.py**: This module is responsible for fetching content from GitHub.\n","* **llm_analyzer.py**: This module contains the core logic for interacting with the LLM to perform the analysis.\n","* **app_core.py**: the main entry point of the application. Imports other modules, defines the UI, and orchestrate the logic."]},{"cell_type":"markdown","metadata":{"id":"HF6iju1dHqdx"},"source":["`config.py: A list of models (to be used for the dropdown)`"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89,"status":"ok","timestamp":1755956298941,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"HSPSD_BhEH_2","outputId":"1f16ad7a-5a10-4565-e5d4-f50227f4d00d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing config.py\n"]}],"source":["# --- Create config.py ---\n","%%writefile config.py\n","\n","# A curated list of popular and effective models for the dropdown\n","CURATED_MODEL_LIST = [\n","    \"llama3:8b\",          # Meta's latest 8B model (Default)\n","    \"gemma3:270M\",         # google Gemma 270M\n","    \"gpt-oss:20b\",        # OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.\n","    \"deepseek-coder:6.7b\",# A top-tier coding model.\n","    \"mistral:7b\",         # Mistral's popular 7B model\n","    \"gemma2:9b\",          # Google's latest 9B model\n","    \"phi3:3.8b\",          # Microsoft's small, powerful model\n","    \"qwen2:7b\",           # Alibaba's latest model\n","    \"codellama:7b\",       # Code-specialized model\n","    \"starcoder2:3b\",      # Another code model from HuggingFace/ServiceNow\n","    \"sqlcoder:7b\",        # Specialized for SQL generation\n","    \"tinydolphin:1.1b\"    # A very small, fast model for quick tests\n","]\n"]},{"cell_type":"markdown","metadata":{"id":"f-GNRPauHwBA"},"source":["`ollama_utils.py: This module handles all direct interactions with the Ollama service.`\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1755956298943,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"C5aqD7b_EQJs","outputId":"fdc8c789-ea50-47f6-8e86-bab5bb48dbaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing ollama_utils.py\n"]}],"source":["# --- Create ollama_utils.py ---\n","%%writefile ollama_utils.py\n","\n","import ollama\n","from typing import List\n","\n","# This module handles all direct interactions with the Ollama service.\n","\n","def get_local_models() -\u003e List[str]:\n","    \"\"\"Gets the list of models already pulled locally.\"\"\"\n","    try:\n","        models_info = ollama.list()\n","        return [model['model'] for model in models_info['models']]\n","    except Exception:\n","        return []\n","\n","def ensure_model_is_pulled(model_name: str):\n","    \"\"\"Checks if a model is available locally, and pulls it if not.\"\"\"\n","    local_models = get_local_models()\n","    if model_name not in local_models:\n","        try:\n","            print(f\"Model '{model_name}' not found locally. Pulling from Ollama...\")\n","            ollama.pull(model_name)\n","            print(\"Model pulled successfully.\")\n","        except Exception as e:\n","            # Raise a standard error that the main app function will catch and display.\n","            raise RuntimeError(f\"Failed to pull model '{model_name}'. Please check the model name and your connection. Details: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"LgYdfcGQIHNh"},"source":["`github_utils.py: This module is responsible for fetching content from GitHub.`"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1755956298944,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"5awj99F9EQ_y","outputId":"715d6974-4bb5-4aff-fb2f-b5daadb735c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing github_utils.py\n"]}],"source":["# --- Create github_utils.py ---\n","%%writefile github_utils.py\n","\n","# This module is responsible for fetching content from GitHub.\n","import requests\n","\n","def get_readme_content(github_url: str) -\u003e str:\n","    \"\"\"Fetches and cleans the README content from a GitHub repo URL.\"\"\"\n","    try:\n","        # To get the raw README, we can adjust the URL\n","        # e.g., https://github.com/user/repo -\u003e https://raw.githubusercontent.com/user/repo/main/README.md\n","        if \"github.com\" not in github_url:\n","            return \"Error\u003crepoAnalyzerAgent\u003e: Please provide a valid GitHub URL.\"\n","\n","        parts = github_url.strip(\"/\").split(\"/\")\n","        user = parts[-2]\n","        repo = parts[-1]\n","\n","        # Try common main branch names\n","        for branch in [\"main\", \"master\",\"develop\", \"dev\"]:\n","            raw_url = f\"https://raw.githubusercontent.com/{user}/{repo}/{branch}/README.md\"\n","            response = requests.get(raw_url)\n","            if response.status_code == 200:\n","                return response.text\n","\n","        return \"Error\u003crepoAnalyzerAgent\u003e: Could not find README.md in 'main' or 'master' branch.\"\n","\n","    except Exception as e:\n","        return f\"Error\u003crepoAnalyzerAgent\u003e fetching content: {e}\"\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5LPqwuMmIQKB"},"source":["`llm_analyzer.py: This module contains the core logic for interacting with the LLM to perform the analysis.`"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1755956298946,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"fm5RmUR9ERMS","outputId":"b83fdd14-0af5-4a37-ab88-5ca7ce94b39a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing llm_analyzer.py\n"]}],"source":["# --- Create llm_analyzer.py ---\n","%%writefile llm_analyzer.py\n","\n","#This module contains the core logic for interacting with the LLM to perform the analysis.\n","import ollama\n","import time\n","from typing import Tuple\n","\n","def analyze_repo_with_llm(context: str, model_name: str) -\u003e Tuple[str, str]:\n","    \"\"\"Uses a local LLM via Ollama to analyze the repo content.\"\"\"\n","\n","    prompt = f\"\"\"\n","    You are an expert senior AI researcher with 20 years of experience.\n","    Your task is to analyze the following GitHub repository's README file and provide a structured, concise, and insightful summary.\n","\n","    **README Content:**\n","    ---\n","    {context}\n","    ---\n","\n","    **Your Analysis:**\n","    Provide the output in the following markdown format:\n","\n","    ### üöÄ Project Summary\n","    (A brief, one-paragraph summary of the project's main goal and functionality.)\n","\n","    ### üõ†Ô∏è Key Technologies \u0026 Libraries\n","    (A bulleted list of the primary technologies, languages, and libraries mentioned or implied.)\n","\n","    ### üí° Potential Use Cases\n","    (A bulleted list of 2-3 potential real-world applications for this project.)\n","\n","    ### üìà Complexity\n","    (Your expert opinion on the project's complexity: Beginner, Intermediate, or Advanced.)\n","    \"\"\"\n","\n","    try:\n","        print(\"-\u003e Sending request to LLM...\")\n","        start_time = time.time() # Start the timer\n","        response = ollama.chat(\n","            model= model_name,  # Or 'gemma2', 'llamma3', 'gemma3:1B', 'gemma3:270M'\n","            messages=[{'role': 'user', 'content': prompt}]\n","        )\n","        end_time = time.time() # End the timer\n","        print(\"-\u003e Received response from LLM.\")\n","        duration = end_time - start_time\n","        llm_response = response['message']['content']\n","        # Append the timing information to the response for display\n","        timing_info = f\"*LLM processing time: {duration:.2f} seconds.*\"\n","\n","        print(\"-\u003e [*] \"+timing_info[1:-1])\n","        return llm_response, timing_info\n","\n","    except Exception as e:\n","        print(f\"Error communicating with LLM: {e})\")\n","        return f\"Error \u003crepoAnalyzerAgent\u003e communicating with LLM {model_name}: {e}\", \"\""]},{"cell_type":"markdown","metadata":{"id":"Pv4Y9gbtIfx4"},"source":["`app_core.py: the main entry point of the application. Imports other modules, defines the UI, and orchestrate the logic.`"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1755957374331,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"H0_ajes1EfbR","outputId":"18839751-41f1-4e14-9977-c061ff4b3873"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting app_core.py\n"]}],"source":["# --- Create app_core.py ---\n","%%writefile app_core.py\n","\n","# This is the main entry point of the application. Imports other modules, defines the UI, and orchestrate the logic.\n","import gradio as gr\n","from typing import Tuple\n","import argparse\n","\n","# Import functions from our other modules\n","from config import CURATED_MODEL_LIST\n","from ollama_utils import ensure_model_is_pulled\n","from github_utils import get_readme_content\n","from llm_analyzer import analyze_repo_with_llm\n","\n","\n","\n","def analyze_github_repo(url: str, model_name: str, progress = gr.Progress(track_tqdm=True)) -\u003e Tuple[str, str]:\n","    \"\"\"The main function that ties everything together for the UI.\"\"\"\n","    if not url or not model_name:\n","        return \"Please enter a GitHub repository URL to begin and select a model.\", \"Please enter a GitHub repository URL to begin and select a model.\"\n","\n","    # step 0: start\n","    msg_0 = \"Initiating github repo anlysis...\"\n","    if isinstance(progress, gr.Progress):\n","        progress(0, desc=msg_0)\n","    else:\n","        print(msg_0)\n","\n","\n","    # Step 1: Ensure the model is available\n","    msg_1 = f\"Step (1/4) Checking model: {model_name}...\"\n","    try:\n","        if isinstance(progress, gr.Progress):\n","            progress(0.25, desc=msg_1)\n","        else:\n","            print(msg_1)\n","        ensure_model_is_pulled(model_name)\n","    except (gr.Error, RuntimeError) as e:\n","        return str(e), str(e)\n","\n","    # Step 2: Fetch README content\n","    msg_2 = f\"Step (2/4) Fetching content for: {url}...\"\n","    if isinstance(progress, gr.Progress):\n","        progress(0.5, desc=msg_2)\n","    else:\n","        print(msg_2)\n","    readme_content = get_readme_content(url)\n","    if \"Error\u003crepoAnalyzerAgent\u003e\" in readme_content:\n","        return readme_content, readme_content\n","\n","    # Step 3: Analyze with the LLM\n","    msg_3 = f\"Step (3/4) Analyzing content with LLM -\u003e {model_name}...\"\n","    if isinstance(progress, gr.Progress):\n","        progress(0.75, desc=msg_3)\n","    else:\n","        print(msg_3)\n","    analysis, timing_info = analyze_repo_with_llm(readme_content, model_name)\n","    #print(timing_info)\n","\n","    # Step 4: Done\n","    msg_4 = \"Step (4/4) Done!\"\n","    if isinstance(progress, gr.Progress):\n","        progress(1, desc= msg_4)\n","    else:\n","        print(msg_4)\n","    return analysis, timing_info\n","\n","\n","def clear_fields():\n","    \"\"\"Returns empty values to clear all input and output fields.\"\"\"\n","    # Clears: url_input, analysis_output, time_output, model_dropdown\n","    return \"\", \"\", \"\", CURATED_MODEL_LIST[0]\n","\n","def create_ui():\n","    \"\"\"Creates the layout and returns the Gradio UI Blocks\"\"\"\n","    with gr.Blocks() as iface:\n","        gr.Markdown(\"# ü§ñ GitHub Repo Analyzer Agent\")\n","        gr.Markdown(\"Enter a GitHub repo URL to get an expert analysis from a local LLM. The first analysis with a model may take some time as the model loads into memory.\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=3):\n","                url_input = gr.Textbox(\n","                    lines=15,\n","                    placeholder=\"e.g., https://github.com/ollama/ollama\",\n","                    label=\"GitHub Repository URL\"\n","                )\n","            with gr.Column(scale=1):\n","                model_dropdown = gr.Dropdown(\n","                    choices=CURATED_MODEL_LIST,\n","                    value=CURATED_MODEL_LIST[0],\n","                    label=\"Select or Enter a LLM Model Name\",\n","                    allow_custom_value=True\n","                )\n","                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n","                stop_btn = gr.Button(\"Stop\", variant=\"stop\")\n","                clear_btn = gr.Button(\"Clear\")\n","                time_output = gr.Textbox(label=\"Processing information\", elem_id=\"time-output\")\n","\n","        with gr.Row():\n","            analysis_output = gr.Markdown(label=\"Expert Analysis\", elem_id=\"analysis-output\")\n","\n","        gr.Examples(\n","            examples=[\n","                [\"https://github.com/openai/gpt-oss\"],\n","                [\"https://github.com/facebookresearch/llama\"],\n","                [\"https://github.com/huggingface/transformers\"],\n","                [\"https://github.com/matlab-deep-learning/llms-with-matlab\"],\n","            ],\n","            inputs=url_input\n","        )\n","\n","        # Event handling\n","        analysis_event = submit_btn.click(\n","            fn=analyze_github_repo,\n","            inputs=[url_input, model_dropdown],\n","            outputs=[analysis_output, time_output]\n","        )\n","        stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[analysis_event])\n","        clear_btn.click(fn=clear_fields, inputs=None, outputs=[url_input, analysis_output, time_output, model_dropdown])\n","\n","    return iface\n","\n","\n","def main_cli():\n","    \"\"\"Handles the command-line interface logic.\"\"\"\n","    parser = argparse.ArgumentParser(description=\"GitHub Repo Analyzer CLI\")\n","    parser.add_argument(\"--url\", required=True, type=str, help=\"URL of the GitHub repository to analyze.\")\n","    parser.add_argument(\"--model\", type=str, default=CURATED_MODEL_LIST[0], help=f\"Name of the Ollama model to use (default: {CURATED_MODEL_LIST[0]}).\")\n","    args = parser.parse_args()\n","\n","    print(\"--- GitHub Repo Analyzer CLI ---\")\n","    analysis, timing_info = analyze_github_repo(args.url, args.model, progress=None)\n","    print(\"\\n--- Analysis Result ---\")\n","    print(analysis)\n","    print(\"-----------------------\")\n","    print(f\"Processing Time: {timing_info}\")\n","    print(\"-----------------------\")\n","\n","\n","\n","if __name__ == \"__main__\":\n","    import sys\n","    # Check if any command-line arguments for the CLI were provided\n","    if len(sys.argv) \u003e 1 and any(arg.startswith('--') for arg in sys.argv):\n","        main_cli()\n","    else:\n","        # If no CLI arguments, launch the Gradio UI\n","        app_ui = create_ui()\n","        print(\"Launching Gradio Interface...\")\n","        app_ui.launch(debug=True, share=False)\n"]},{"cell_type":"markdown","metadata":{"id":"WOTWc5N9DnOQ"},"source":["# Prep-pull a default Language Model"]},{"cell_type":"markdown","metadata":{"id":"C2loKavZIyn-"},"source":["Pull a default model (e.g llama3:8b) to ensure the app is ready to use immediately after setup. This may take a few minutes."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":63518,"status":"ok","timestamp":1755956378963,"user":{"displayName":"Moubarak el pablo","userId":"08722843378766923106"},"user_tz":-120},"id":"PMJaDsCmFYdY","outputId":"286cf9d0-33fb-476c-924c-975985bace80"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚è≥ Pulling the default model (llama3:8b)...\n","‚úÖ Default model pulled successfully!\n"]}],"source":["import ollama\n","\n","try:\n","    print(\"‚è≥ Pulling the default model (llama3:8b)...\")\n","    ollama.pull('llama3:8b')\n","    print(\"‚úÖ Default model pulled successfully!\")\n","except Exception as e:\n","    print(f\"‚ùóÔ∏è An error occurred while pulling the model: {e}\")\n","    print(\"üìÑ Check the ollama.log for more details by running !cat ollama.log\")"]},{"cell_type":"markdown","metadata":{"id":"LWAdRuzVD2IV"},"source":["# Run the repo analyzer application"]},{"cell_type":"markdown","metadata":{"id":"43xhUTaeFk5I"},"source":["Execute the main Python script to launch the Gradio app."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0SPj_5aFFwhd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Launching Gradio Interface...\n","* Running on local URL:  http://127.0.0.1:7860\n","* To create a public link, set `share=True` in `launch()`.\n"]}],"source":["!python app_core.py"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyP4m/+oCcKDr2pOjzDBOAxP","name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}